
# Introduction  
This code investigates the effectiveness of large language models (LLMs) in transforming causal domain knowledge into a representation that better aligns with recommendations from causal data science.

## method

The approach consists of two main steps. 

### Nodes in the causal diagram represents causal variables not its realized values
The first step involves identifying if two entries, gathered through brainstorming approaches such as using cognitive maps or extracted using automated causal information extraction method, describe the values of the same causal variable.
Our method investigates whether giving two texts, which could represent a cause or effect of different causal relations, actually represents different values of the same causal variable. 
To test this, a prompt has been designed for the purpose. 
To compare and test different LLMs' effectiveness in achieving these tasks, another prompt has been utilized to generate data.
The generated data consists of a specified number of causal variables for a selected domain and several examples of their realized values.
The LLMs have been instructed to provide examples of realized values using noun phrases and to avoid ambiguous values such as 'high' or 'low,' as well as numerical values. 
To generate your oun data  you can use the following script 
```bash
python src/CMR1/CMR1_data_generation.py
```

The generated data are then sampled for positive and negative examples based on the generated models and the domain. 
A pair of two texts is considered to represent different values of the same causal variable if they occurred under the variable example of the generated lists.
Tha data sampling script can be run under 

```bash
python src/CMR1/CMR1_data_sampling.py
```

Next the LLMs are instructed to predict each sample of the sampled data and compared withe the actual values generated by the LLMs
The code for running the experiments can be run under 

```bash
python src/CMR1/CMR1_Expirment.py
```

### Interaction events, that give information about more than one causal variable, are the result of their interaction and should be modeled as artificial nodes

Brainstorming approaches for capturing causal domain knowledge, such as cognitive mapping,  relies on human subject matter experts in articulating the causes and effects of a certain  concept.
Thus, the causes and effects are articulated as events. 
In some cases, an event can describe an interaction between two or more causal variables. 
For instance, an interaction event denoted as $ie$ describes an interaction between causal variable $F$ and variable $E$. 
Specifically, the interaction event $ie$ corresponds to the simultaneous occurrence of observed value $f_1$ for causal variable $F$ and observed value $e_1$ for variable $E$.
It is important to note that such events should not be included in either causal variable $F$ or $E$ to maintain the modularity of these variables. 
Instead, representing the event $ie$ in the causal diagram as an artificial node is recommended.
This artificial node should encompass the corresponding observed values it represents; in this example, $f_1$ and $e_1$.
Furthermore, the artificial node can be linked to the causal variables that encompass the observed values of this interaction event. 
However, it is crucial to highlight that the type of this relation should differ from other causal relations in diagrams. 
Specifically, the relation type is not a consistent causal relation, as causal diagrams are defined to link two causal variables and not a causal variable to events represented by artificial nodes.
This approach aligns with the technique proposed by VanderWeele 2009 for modeling interactions between two or more causal variables in causal diagrams.


